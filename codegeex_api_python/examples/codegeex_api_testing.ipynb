{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd92436",
   "metadata": {},
   "source": [
    "Ref:  \n",
    "- [LLMs: Get predictions from a language model](https://python.langchain.com/en/latest/getting_started/getting_started.html#llms-get-predictions-from-a-language-model)\n",
    "- [A simple guide to setting the GPT-3 temperature](https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be)\n",
    "- [HumanEval-X: A new benchmark for Multilingual Program Synthesis](https://github.com/THUDM/CodeGeeX/blob/main/codegeex/benchmark/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcf5da",
   "metadata": {},
   "source": [
    "## Import HumanEval-X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9736d9-2761-4995-8fdd-85a7c42ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpp:  164 taske(s), elapsed_time: 2995.141745413188 S, 49.9190290902198 minutes, 0.8319838181703301 hours.\n",
    "# java: 164 taske(s), elapsed_time: 4135.389279692899 S, 68.92315466154832 minutes, 1.1487192443591387 hours.\n",
    "# python: 164 taske(s), elapsed_time: 7769.483070801012 S, 129.4913845133502 minutes, 2.15818974188917 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07d3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = ('cpp', 'java', 'python')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea86e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b75c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path('.').resolve().parents[0]))\n",
    "\n",
    "from codegeex_api import CodeGeeX\n",
    "from codegeex_utility import stream_jsonl_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706913c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"CodeGeeX_home\": \"/root/autodl-tmp/GitHub/CodeGeeX\",\n",
      "    \"CoderEval_home\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "manage_properties = dict([\n",
    "        (line.split('=')[0], line.split('=')[1].strip())\n",
    "            for line in open(f'../../manage.properties')])\n",
    "\n",
    "print(json.dumps(manage_properties, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "659112ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Human Evalu Python\n",
    "HEP_FILE = os.path.join(\n",
    "        manage_properties['CodeGeeX_home'],\n",
    "        f'codegeex/benchmark/humaneval-x/{language}' \\\n",
    "                f'/data/humaneval_{language}.jsonl.gz')\n",
    "assert os.path.exists(HEP_FILE)\n",
    "\n",
    "HEP = stream_jsonl_all(HEP_FILE)\n",
    "\n",
    "print(len(HEP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1978d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = HEP[0]['task_id'].split('/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b116c80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGUAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efecfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = HEP[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de55b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e204b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d3e7b",
   "metadata": {},
   "source": [
    "## Ask CodeGeeX to generate code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a5a55",
   "metadata": {},
   "source": [
    "#### <font size=\"7\" color=\"orange\">âš </font> Do <span style=\"color:red\">NOT</span> submit the config file to GitHub because of security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de2d60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "codegeex_api_config = json.load(\n",
    "        open('codegeex_api_config.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa8577c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'api_base': 'http://localhost:8080/v2',\n",
       " 'api_version': '2.1.0.0',\n",
       " 'api_type': 'codegeex',\n",
       " 'api_key': 'D4B94CC818A3D8A725CCC8FE68B97'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codegeex_api_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68be839-09af-4c91-a600-6621d2ac6425",
   "metadata": {},
   "source": [
    "**For \"1.x.x.x\", return string; for \"2.x.x.x\", return json.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c24a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CodeGeeX(codegeex_api_config)  # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c288e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_return = m(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba5b674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stdout': ['    return any(n1 == n2 or abs(n1 - n2) < threshold for n1, n2 in zip(numbers[:-1],\\n                                                                       numbers[1:]))\\n\\n\\ndef split_field_into_chunks(fields: List[List[str]], chunk_size: int, pad_token: str = \\'<pad>\\') -> Tuple[List[List[str]], List[str]]:\\n    \"\"\" Split a list of fields, such that each chunk has size chunk_size.\\n    >>> split_field_into_chunks([\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\'], 2)\\n    ([[\\'a\\', \\'b\\'], [\\'c\\', \\'d\\', \\'e\\', \\'f\\', \\'g\\']], [\\'<pad>\\', \\'<pad>\\', \\'<pad>\\', \\'<pad>\\', \\'<pad>\\', \\'<pad>\\'])\\n    \"\"\"\\n    pad_token_id = int(spacy_nlp(pad_token)[0].vocab.stoi[pad_token])\\n    max_length = max(map(len, fields))\\n\\n    chunks = []\\n    for i in range(0, len(fields), chunk_size):\\n        chunk = []\\n        for j, field in enumerate(fields[i:i + chunk_size]):\\n            chunk.append(field)\\n            chunk.append(pad_token)\\n            chunk.append(pad_token)\\n        chunk = chunk[:-2]\\n        chunks.append(chunk)\\n\\n    labels = [\\'\\'] * len(chunks)\\n    for i, chunk in enumerate(chunks):\\n        for j, word in enumerate(chunk[:-1]):\\n            labels[i * chunk_size + j] = chunk[j]\\n\\n    return chunks, labels\\n\\n\\ndef convert_string_tokens_to_ids(tokens: List[str], vocab: Dict[str, int], max_length: int = None) -> List[int]:\\n    \"\"\" Convert a list of strings to a list of ids.\\n    >>> vocab = {\\'<pad>\\': 0, \\'a\\': 1, \\'b\\': 2, \\'c\\': 3}\\n    >>> convert_string_tokens_to_ids([\\'a\\', \\'b\\', \\'c\\'], vocab)\\n    [1, 2, 3]\\n    >>> convert_string_tokens_to_ids([\\'a\\', \\'b\\', \\'c\\'], vocab, max_length=3)\\n    [1, 2, 3]\\n    \"\"\"\\n    if max_length is None:\\n        return [int(vocab.get(token, vocab[\\'<unk>\\'])) for token in tokens]\\n\\n    ids = []\\n    for token in tokens:\\n        ids.append(int(vocab.get(token, vocab[\\'<unk>\\'])))\\n        ids.append(int(vocab.get(token, vocab[\\'<unk>\\'])))\\n        if len(ids) == max_length:\\n            break\\n\\n    return ids\\n\\n\\ndef convert_ids_to_tokens(ids: List[int], vocab: Dict[str, int], join: bool = True) -> List[str]:\\n    \"\"\" Convert a list of ids to a list of tokens.\\n    >>> vocab = {\\'<pad>\\': 0, \\'a\\': 1, \\'b\\': 2, \\'c\\': 3}\\n    >>> convert_ids_to_tokens([1, 2, 3], vocab)\\n    [\\'a\\', \\'b\\', \\'c\\']\\n    >>> convert_ids_to_tokens([1, 2, 3], vocab, join=False)\\n    [\\'a\\', \\'b\\', \\'c\\']\\n    \"\"\"\\n    tokens = []\\n    for i in ids:\\n        '], 'stderr': '', 'elapsed_time': 48.72319067502394}\n"
     ]
    }
   ],
   "source": [
    "print(m_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a7f35c-0ea8-4c7c-91e7-0ab96247a5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"elapsed_time\": 48.72319067502394,\n",
      "    \"stderr\": \"\",\n",
      "    \"stdout\": [\n",
      "        \"    return any(n1 == n2 or abs(n1 - n2) < threshold for n1, n2 in zip(numbers[:-1],\\n                                                                       numbers[1:]))\\n\\n\\ndef split_field_into_chunks(fields: List[List[str]], chunk_size: int, pad_token: str = '<pad>') -> Tuple[List[List[str]], List[str]]:\\n    \\\"\\\"\\\" Split a list of fields, such that each chunk has size chunk_size.\\n    >>> split_field_into_chunks(['a', 'b', 'c', 'd', 'e', 'f', 'g'], 2)\\n    ([['a', 'b'], ['c', 'd', 'e', 'f', 'g']], ['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'])\\n    \\\"\\\"\\\"\\n    pad_token_id = int(spacy_nlp(pad_token)[0].vocab.stoi[pad_token])\\n    max_length = max(map(len, fields))\\n\\n    chunks = []\\n    for i in range(0, len(fields), chunk_size):\\n        chunk = []\\n        for j, field in enumerate(fields[i:i + chunk_size]):\\n            chunk.append(field)\\n            chunk.append(pad_token)\\n            chunk.append(pad_token)\\n        chunk = chunk[:-2]\\n        chunks.append(chunk)\\n\\n    labels = [''] * len(chunks)\\n    for i, chunk in enumerate(chunks):\\n        for j, word in enumerate(chunk[:-1]):\\n            labels[i * chunk_size + j] = chunk[j]\\n\\n    return chunks, labels\\n\\n\\ndef convert_string_tokens_to_ids(tokens: List[str], vocab: Dict[str, int], max_length: int = None) -> List[int]:\\n    \\\"\\\"\\\" Convert a list of strings to a list of ids.\\n    >>> vocab = {'<pad>': 0, 'a': 1, 'b': 2, 'c': 3}\\n    >>> convert_string_tokens_to_ids(['a', 'b', 'c'], vocab)\\n    [1, 2, 3]\\n    >>> convert_string_tokens_to_ids(['a', 'b', 'c'], vocab, max_length=3)\\n    [1, 2, 3]\\n    \\\"\\\"\\\"\\n    if max_length is None:\\n        return [int(vocab.get(token, vocab['<unk>'])) for token in tokens]\\n\\n    ids = []\\n    for token in tokens:\\n        ids.append(int(vocab.get(token, vocab['<unk>'])))\\n        ids.append(int(vocab.get(token, vocab['<unk>'])))\\n        if len(ids) == max_length:\\n            break\\n\\n    return ids\\n\\n\\ndef convert_ids_to_tokens(ids: List[int], vocab: Dict[str, int], join: bool = True) -> List[str]:\\n    \\\"\\\"\\\" Convert a list of ids to a list of tokens.\\n    >>> vocab = {'<pad>': 0, 'a': 1, 'b': 2, 'c': 3}\\n    >>> convert_ids_to_tokens([1, 2, 3], vocab)\\n    ['a', 'b', 'c']\\n    >>> convert_ids_to_tokens([1, 2, 3], vocab, join=False)\\n    ['a', 'b', 'c']\\n    \\\"\\\"\\\"\\n    tokens = []\\n    for i in ids:\\n        \"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(m_return, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9d90b-0c3b-429f-a6a8-b4af817cd0ef",
   "metadata": {},
   "source": [
    "**temperature & top_p**  \n",
    "\n",
    "Ref:  \n",
    "- Zheng, Q., Xia, X., Zou, X., Dong, Y., Wang, S., Xue, Y., Wang, Z., Shen, L., Wang, A., Li, Y. and Su, T., 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. *arXiv preprint arXiv:2303.17568*.\n",
    "\n",
    "#### 4.1 Evaluation Settings\n",
    "Page 11:\n",
    "\n",
    "*For CodeGeeX in code generation, we use t = 0:2; p = 0:95 for pass@1 and\n",
    "t = 0:8; p = 0:95 for pass@10 and pass@100 (except for Go and JavaScript, where p = 0:9).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad71875-de2a-44b8-a018-526c18ed7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_return = m(PROMPT, temperature=0.2, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cffd203-16ac-418e-85f2-a59fc8de5348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"elapsed_time\": 48.9645616482012,\n",
      "    \"stderr\": \"\",\n",
      "    \"stdout\": [\n",
      "        \"    for i in range(len(numbers) - 1):\\n        if abs(numbers[i] - numbers[i + 1]) > threshold:\\n            return True\\n    return False\\n\\n\\ndef get_number_of_close_elements(numbers: List[float], threshold: float) -> int:\\n    \\\"\\\"\\\" Return the number of elements in given list of numbers, that are closer to each\\n    other than given threshold.\\n    >>> get_number_of_close_elements([1.0, 2.0, 3.0], 0.5)\\n    1\\n    >>> get_number_of_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    2\\n    \\\"\\\"\\\"\\n    count = 0\\n    for i in range(len(numbers) - 1):\\n        if abs(numbers[i] - numbers[i + 1]) > threshold:\\n            count += 1\\n    return count\\n\\n\\ndef get_number_of_close_elements_in_list(numbers: List[float], threshold: float) -> int:\\n    \\\"\\\"\\\" Return the number of elements in given list of numbers, that are closer to each\\n    other than given threshold.\\n    >>> get_number_of_close_elements_in_list([1.0, 2.0, 3.0], 0.5)\\n    1\\n    >>> get_number_of_close_elements_in_list([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    2\\n    \\\"\\\"\\\"\\n    count = 0\\n    for i in range(len(numbers) - 1):\\n        if abs(numbers[i] - numbers[i + 1]) > threshold:\\n            count += 1\\n    return count\\n\\n\\ndef get_number_of_close_elements_in_list_with_index(\\n    numbers: List[float], threshold: float\\n) -> Tuple[int, List[int]]:\\n    \\\"\\\"\\\" Return the number of elements in given list of numbers, that are closer to each\\n    other than given threshold.\\n    >>> get_number_of_close_elements_in_list_with_index([1.0, 2.0, 3.0], 0.5)\\n    (1, [0])\\n    >>> get_number_of_close_elements_in_list_with_index([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    (2, [1, 3])\\n    \\\"\\\"\\\"\\n    count = 0\\n    index = []\\n    for i in range(len(numbers) - 1):\\n        if abs(numbers[i] - numbers[i + 1]) > threshold:\\n            count += 1\\n            index.append(i)\\n    return count, index\\n\\n\\ndef get_number_of_close_elements_in_list_with_index_and_value(\\n    numbers: List[float], threshold: float\\n) -> Tuple[int, List[int], float]:\\n    \\\"\\\"\\\" Return the number of elements in given list of numbers, that are closer to each\\n    other than given threshold.\\n    >>> get_number_of_close_elements_in_list_with_index_and_value([1.0, 2.0, 3.0], 0.5)\\n    (1, [0], 1.0)\\n    >>> get_number_of_close_elements_in_list_with_index_and_value([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    (2, [1, 3], 2.8)\\n    \\\"\\\"\\\"\\n    count = 0\\n    index = []\\n    value = 0.0\\n    for i\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(m_return, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7b49092-f5d9-46fd-9680-a8166e554d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose=True will let more info print out to the sever's stderr.\n",
    "# However, client will not get these info.\n",
    "\n",
    "m_return = m(PROMPT, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481df048-f24a-4517-92ec-c858f37e4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"elapsed_time\": 48.85856589395553,\n",
      "    \"stderr\": \"\",\n",
      "    \"stdout\": [\n",
      "        \"    assert len(numbers) > 0, \\\"No numbers given\\\"\\n    assert threshold > 0, \\\"Threshold should be positive\\\"\\n    for i in range(len(numbers) - 1):\\n        assert numbers[i + 1] > numbers[i], \\\"Numbers should be sorted\\\"\\n        if abs(numbers[i + 1] - numbers[i]) < threshold:\\n            return True\\n    return False\\n\\n\\ndef _build_vocab(embeddings_path: str, vocab_file: str,\\n                 build_vocab_from_words: bool = True,\\n                 min_count: int = 3,\\n                 out_dir: str = 'vocabs'):\\n    \\\"\\\"\\\"\\n    Build vocabulary for given path to embeddings file.\\n    If build_vocab_from_words is true, then builds a vocab from words of the embeddings file,\\n    else builds a vocab from lines of the embeddings file.\\n\\n    :param embeddings_path: Path to the embeddings file\\n    :param vocab_file: File where to store vocab\\n    :param build_vocab_from_words: If True, builds a vocab from words of the embeddings file,\\n                                 else builds a vocab from lines of the embeddings file.\\n    :param min_count: Minimum count to consider a word in the vocab\\n    :param out_dir: Output directory\\n    :return: None\\n    \\\"\\\"\\\"\\n    embeddings_in = io.open(embeddings_path, 'r', encoding='utf-8')\\n    embeddings_out = io.open(os.path.join(out_dir, os.path.basename(embeddings_path)), 'w',\\n                             encoding='utf-8')\\n    counter = collections.Counter()\\n\\n    for line in embeddings_in:\\n        word = line.strip().split(' ')[0]\\n        counter.update([word])\\n    embeddings_in.close()\\n\\n    if build_vocab_from_words:\\n        for word in counter.most_common():\\n            if word[1] >= min_count:\\n                embeddings_out.write(word[0] + '\\\\n')\\n    else:\\n        with open(embeddings_path) as f:\\n            for line in f:\\n                word = line.strip().split(' ')[0]\\n                counter.update([word])\\n        for word in counter.most_common():\\n            if word[1] >= min_count:\\n                embeddings_out.write(word[0] +'')\\n                embeddings_out.write(' '.join(str(float(num)) for num in line.strip().split(' ')[1:]))\\n                embeddings_out.write('\\\\n')\\n    embeddings_out.close()\\n\\n\\ndef _build_vocab_from_txt(vocab_file: str,\\n                            build_vocab_from_words: bool = True,\\n                            min_count: int = 3,\\n                            out_dir: str = 'vocabs'):\\n    \\\"\\\"\\\"\\n    Build vocabulary for given path to embeddings file.\\n    If build_vocab_from_words is true, then builds a vocab from words of the embeddings file,\\n    else builds a vocab from lines of the embeddings file.\\n\\n    :param embeddings_path: Path to the embeddings file\\n    :param vocab_file: File where to store vocab\\n    :param build_vocab_from_words: If True, builds a vocab from words of the embeddings file,\\n                                 else builds a vocab from lines of the embeddings file.\\n    :param min_count: Minimum count to consider a word in the vocab\\n    :param out_\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(m_return, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9832a68f-c2a0-4b90-a6f2-d0d26574b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    assert len(numbers) > 0, \"No numbers given\"\n",
      "    assert threshold > 0, \"Threshold should be positive\"\n",
      "    for i in range(len(numbers) - 1):\n",
      "        assert numbers[i + 1] > numbers[i], \"Numbers should be sorted\"\n",
      "        if abs(numbers[i + 1] - numbers[i]) < threshold:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "\n",
      "def _build_vocab(embeddings_path: str, vocab_file: str,\n",
      "                 build_vocab_from_words: bool = True,\n",
      "                 min_count: int = 3,\n",
      "                 out_dir: str = 'vocabs'):\n",
      "    \"\"\"\n",
      "    Build vocabulary for given path to embeddings file.\n",
      "    If build_vocab_from_words is true, then builds a vocab from words of the embeddings file,\n",
      "    else builds a vocab from lines of the embeddings file.\n",
      "\n",
      "    :param embeddings_path: Path to the embeddings file\n",
      "    :param vocab_file: File where to store vocab\n",
      "    :param build_vocab_from_words: If True, builds a vocab from words of the embeddings file,\n",
      "                                 else builds a vocab from lines of the embeddings file.\n",
      "    :param min_count: Minimum count to consider a word in the vocab\n",
      "    :param out_dir: Output directory\n",
      "    :return: None\n",
      "    \"\"\"\n",
      "    embeddings_in = io.open(embeddings_path, 'r', encoding='utf-8')\n",
      "    embeddings_out = io.open(os.path.join(out_dir, os.path.basename(embeddings_path)), 'w',\n",
      "                             encoding='utf-8')\n",
      "    counter = collections.Counter()\n",
      "\n",
      "    for line in embeddings_in:\n",
      "        word = line.strip().split(' ')[0]\n",
      "        counter.update([word])\n",
      "    embeddings_in.close()\n",
      "\n",
      "    if build_vocab_from_words:\n",
      "        for word in counter.most_common():\n",
      "            if word[1] >= min_count:\n",
      "                embeddings_out.write(word[0] + '\\n')\n",
      "    else:\n",
      "        with open(embeddings_path) as f:\n",
      "            for line in f:\n",
      "                word = line.strip().split(' ')[0]\n",
      "                counter.update([word])\n",
      "        for word in counter.most_common():\n",
      "            if word[1] >= min_count:\n",
      "                embeddings_out.write(word[0] +'')\n",
      "                embeddings_out.write(' '.join(str(float(num)) for num in line.strip().split(' ')[1:]))\n",
      "                embeddings_out.write('\\n')\n",
      "    embeddings_out.close()\n",
      "\n",
      "\n",
      "def _build_vocab_from_txt(vocab_file: str,\n",
      "                            build_vocab_from_words: bool = True,\n",
      "                            min_count: int = 3,\n",
      "                            out_dir: str = 'vocabs'):\n",
      "    \"\"\"\n",
      "    Build vocabulary for given path to embeddings file.\n",
      "    If build_vocab_from_words is true, then builds a vocab from words of the embeddings file,\n",
      "    else builds a vocab from lines of the embeddings file.\n",
      "\n",
      "    :param embeddings_path: Path to the embeddings file\n",
      "    :param vocab_file: File where to store vocab\n",
      "    :param build_vocab_from_words: If True, builds a vocab from words of the embeddings file,\n",
      "                                 else builds a vocab from lines of the embeddings file.\n",
      "    :param min_count: Minimum count to consider a word in the vocab\n",
      "    :param out_\n"
     ]
    }
   ],
   "source": [
    "print(m_return['stdout'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8749d-afdd-4736-90f3-0f68e66d1cf3",
   "metadata": {},
   "source": [
    "**Run 164 and write to jsonl file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ec47126-abff-4b47-8a6f-1ed36f4a2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56dae08-70a9-4b58-9ff9-5cc952699cbe",
   "metadata": {},
   "source": [
    " JSON list format. Ref: https://github.com/THUDM/CodeGeeX/blob/main/codegeex/benchmark/README.md#evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d49f6de3-626a-469e-bd72-a6cd9296d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'results/{language}_t02_p095_{datetime.datetime.now()}' \\\n",
    "        .replace(':', '').replace('-', '').replace('.', '_').replace(' ', '_') \\\n",
    "        + '.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "726d6de8-cb86-438d-af70-30a7dfbf2d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/python_t02_p095_20230713_095221_622978.jsonl'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd18cf8d-c756-4085-83fb-066c9d288d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bca23c2-ac04-4415-9cd9-bda55ed5609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python/0\tPython/1\tPython/2\tPython/3\tPython/4\tPython/5\tPython/6\tPython/7\tPython/8\tPython/9\tPython/10\tPython/11\tPython/12\tPython/13\tPython/14\tPython/15\tPython/16\tPython/17\tPython/18\tPython/19\tPython/20\tPython/21\tPython/22\tPython/23\tPython/24\tPython/25\tPython/26\tPython/27\tPython/28\tPython/29\tPython/30\tPython/31\tPython/32\tPython/33\tPython/34\tPython/35\tPython/36\tPython/37\tPython/38\tPython/39\tPython/40\tPython/41\tPython/42\tPython/43\tPython/44\tPython/45\tPython/46\tPython/47\tPython/48\tPython/49\tPython/50\tPython/51\tPython/52\tPython/53\tPython/54\tPython/55\tPython/56\tPython/57\tPython/58\tPython/59\tPython/60\tPython/61\tPython/62\tPython/63\tPython/64\tPython/65\tPython/66\tPython/67\tPython/68\tPython/69\tPython/70\tPython/71\tPython/72\tPython/73\tPython/74\tPython/75\tPython/76\tPython/77\tPython/78\tPython/79\tPython/80\tPython/81\tPython/82\tPython/83\tPython/84\tPython/85\tPython/86\tPython/87\tPython/88\tPython/89\tPython/90\tPython/91\tPython/92\tPython/93\tPython/94\tPython/95\tPython/96\tPython/97\tPython/98\tPython/99\tPython/100\tPython/101\tPython/102\tPython/103\tPython/104\tPython/105\tPython/106\tPython/107\tPython/108\tPython/109\tPython/110\tPython/111\tPython/112\tPython/113\tPython/114\tPython/115\tPython/116\tPython/117\tPython/118\tPython/119\tPython/120\tPython/121\tPython/122\tPython/123\tPython/124\tPython/125\tPython/126\tPython/127\tPython/128\tPython/129\tPython/130\tPython/131\tPython/132\tPython/133\tPython/134\tPython/135\tPython/136\tPython/137\tPython/138\tPython/139\tPython/140\tPython/141\tPython/142\tPython/143\tPython/144\tPython/145\tPython/146\tPython/147\tPython/148\tPython/149\tPython/150\tPython/151\tPython/152\tPython/153\tPython/154\tPython/155\tPython/156\tPython/157\tPython/158\tPython/159\tPython/160\tPython/161\tPython/162\tPython/163\t\n",
      "164 taske(s), elapsed_time: 7769.483070801012 S, 129.4913845133502 minutes, 2.15818974188917 hours.\n"
     ]
    }
   ],
   "source": [
    "st = time.perf_counter()\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for i in HEP:\n",
    "        print(i['task_id'], end='\\t')\n",
    "        \n",
    "        # Generate code\n",
    "        # use t = 0:2; p = 0:95 for pass@1\n",
    "        m_return = m(i['prompt'], temperature=0.2, top_p=0.95)\n",
    "        \n",
    "        line = {\n",
    "            'task_id': i['task_id'],\n",
    "            'prompt': i['prompt'],\n",
    "            'generation': m_return['stdout'][0].replace('<|endoftext|>', '')\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "\n",
    "et = time.perf_counter()\n",
    "elapsed_time = et - st\n",
    "print(f'\\n{len(HEP)} taske(s), elapsed_time: ' \\\n",
    "        f'{elapsed_time} S, {elapsed_time/60} minutes, {elapsed_time/60/60} hours.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5168ca4-0502-4594-8776-86d096b43265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
